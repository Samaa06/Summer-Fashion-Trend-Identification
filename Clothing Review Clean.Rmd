---
title: "Women Ecommerce Clothing Review Dataset"
author: "Michelle Tirtoatmojo"
date: "2024-07-01"
output: html_document
---

We first started by installing necessary libraries and importing the datasets and renaming it to 'clothing' for easier handling. After exploring the data and gained a better understanding of the datasets, we realized that there are not much missing datasets in the numeric columns (i.e rating). However, we noticed a couple missing values either in the 'Title' or 'Review.Text' column. 

```{r}

# Set CRAN mirror
options(repos = c(CRAN = "https://cran.rstudio.com"))

#Install libraries & load data 
install.packages("readr")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("RColorBrewer")
library(SnowballC)
library(readr)
library(dplyr)
library(tidyr)
library(wordcloud)
library(RColorBrewer)

clothing <- read_csv('Women_Clothing_Reviews.csv')
str(clothing)
dim(clothing)

# Calculate the number of missing values for each column
missing_counts <- sapply(clothing, function(x) sum(is.na(x)))
print(missing_counts)
```

Since some missing values are located in columns where the characters are string - and because there are only a few of these, we decided to omit both where the 'Review.Text' and 'Title' are missing. However, when 'Review.Text' is present, we decided to replace it with N/A values. On top of this, since there are only 13 missing values under column 'Division.Name', 'Department.Name', and 'Class.Name', we decided to replace it with default values (i.e 'Unknown')

```{r clothing}
#Pre-process the data 
#Remove rows where both the title and review text are NA
clothing <- clothing[!(is.na(clothing$Title) & is.na(clothing$Review.Text)), ]

#Replace NA values in the title column with an empty string where the review text is present
clothing$Title[is.na(clothing$Title) & !is.na(clothing$Review.Text)] <- "N/A"

# Fill other missing values with a default value (e.g., "Unknown" for character columns, 0 for numeric columns)
clothing <- clothing %>%
  mutate(
    Division.Name = replace_na(Division.Name, "Unknown"),
    Department.Name = replace_na(Department.Name, "Unknown"),
    Class.Name = replace_na(Class.Name, "Unknown")
  )
```

We then process the Review.Text by first creating a corpus. Then clean the text by converting it to lower case, removing punctuation, removing stopwords, and removing whitespace

```{r pressure, echo=FALSE}
#Create corpus 
install.packages('tm')
library(tm)
corpus = Corpus(VectorSource(clothing$Review.Text))
names(corpus[[1]])

# Convert to lower case
corpus1 <- tm_map(corpus, content_transformer(tolower))
# Remove punctuations 
corpus2 <- tm_map(corpus1, removePunctuation)
# Remove stopwords
corpus3 <- tm_map(corpus2, removeWords, stopwords('english'))
# Remove whitespace
corpus4 <- tm_map(corpus3, stripWhitespace)
# Apply gsub to remove multiple spaces within a content_transformer
corpus4 <- tm_map(corpus4, content_transformer(function(x) gsub("\\s+", " ", x)))
# Stem document
corpus4 <- tm_map(corpus4, stemDocument, language = "english")

# Inspect final corpus 
inspect(corpus4[1:5])
```

Next, we generate a dictionary containing all the words from the Review.Text. After grouping similar words together, we will restore the removed parts using this dictionary. To streamline our analysis, we will filter out terms that appear in less than 5% of the reviews, keeping only those that appear in 5% or more. We will then convert the document-term matrix into a data frame, ensuring the column names (which will be the tokens) are appropriately formatted

```{r}
#Create dictionary 
dict = findFreqTerms(DocumentTermMatrix(Corpus(VectorSource(clothing$Review.Text))),
                     lowfreq = 0)
dict_corpus = Corpus(VectorSource(dict))

#Term frequency weighting 
dtm = DocumentTermMatrix(corpus4)
dtm
inspect(dtm[1000,])
inspect(dtm[617,'dress'])

dim(dtm)
inspect(dtm[1:3, 1:10])

#Remove sparse term 
xdtm = removeSparseTerms(dtm,sparse = 0.95)
xdtm

xdtm = as.data.frame(as.matrix(xdtm))
colnames(xdtm) = stemCompletion(x = colnames(xdtm),
                                dictionary = dict_corpus,
                                type='prevalent')
colnames(xdtm) = make.names(colnames(xdtm))
```

Next, we assess the frequency of each token. These frequencies will be used to assign weights to the tokens, with more common words receiving higher weights compared to less common ones.

```{r}
#Browse token 
sort(colSums(xdtm),decreasing = T)

#Frequency Weighting 
dtm_tfidf = DocumentTermMatrix(x=corpus4,
                               control = list(weighting=function(x) weightTfIdf(x,normalize=F)))
xdtm_tfidf = removeSparseTerms(dtm_tfidf,sparse = 0.95)
xdtm_tfidf = as.data.frame(as.matrix(xdtm_tfidf))
colnames(xdtm_tfidf) = stemCompletion(x = colnames(xdtm_tfidf),
                                      dictionary = dict_corpus,
                                      type='prevalent')
colnames(xdtm_tfidf) = make.names(colnames(xdtm_tfidf))
sort(colSums(xdtm_tfidf),decreasing = T)

xdtm_tfidf[611:620,41:50]

term_freq_tfidf <- colSums(as.matrix(xdtm_tfidf))
term_freq_tfidf_sorted <- sort(term_freq_tfidf, decreasing = TRUE)
print(term_freq_tfidf_sorted)
```

Lastly on understanding our pre-processed data, we visualize the weights of term frequency and term frequency inverse document frequency weighting for the top 20 terms using ggplot's horizontal bar chart contrasts the weights of term frequency and term frequency inverse document frequency weighting for the top 20 terms.

```{r}
library(tidyr); library(dplyr); library(ggplot2); library(ggthemes)
data.frame(term = colnames(xdtm),tf = colMeans(xdtm), tfidf = colMeans(xdtm_tfidf))%>%
  arrange(desc(tf))%>%
  top_n(20)%>%
  gather(key=weighting_method,value=weight,2:3)%>%
  ggplot(aes(x=term,y=weight,fill=weighting_method))+
  geom_col(position='dodge')+
  coord_flip()+
  theme_economist()
```